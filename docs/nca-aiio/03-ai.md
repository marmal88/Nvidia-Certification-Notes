# AI

## Contents
- [CUDA](#1-cuda)

### 1. CUDA

Compute Unified Device Architecture (CUDA) is both a parallel computing platform and a programming model that allows users to use GPUs for general purpose comuting.

#### 1.1 Cuda Python 
Nvidia Cuda python brings GPU compute to python, the de-facto language for modern data science

- cuPy a SciPy and numpy standin for GPU computing with python
- Numba a python compiler by anaconda to compile python code on CUDA capable GPUs

#### 1.2 CUDA-X libraries
A plethora of libraries that brings the power of GPU to everyday compute tasks.

#### 1.2.1 Cuda Math
| CUDA library | Description | Column3 |
| --------------- | --------------- | --------------- |
| cuBLAS | Basic Linear algebra | Item3.1 |
| cuFFT | Fast Fourier transform | Item3.2 |
| cuRAND | Random number generation ||
| cuSOLVER | Dense and sparse direct solvers ||
| cuSPARSE | BLAS for sparse matrices ||
| cuTensor | tensor accelerated linear algebra ||
| cuDSS | direct sparse solver ||
| CUDA Math API | standard math operations ||
| nvmath-python | (beta) standard math operations ||
| AmgX | linear solvers for simulations and implicit unstructured methods ||

##### 1.2.1 RAPIDS
RAPIDS brings GPU to typical data science operations, with drop in libraries for the following frameworks.

| Data science framework | RAPIDS equivalent | docs |
|---|---|---|
| Python | cuDF | [docs](https://docs.rapids.ai/api/cudf/stable/) |
| Numpy & SciPy | cuPy | [docs](https://docs.cupy.dev/en/stable/) |
| Sklearn | cuML | [docs](https://docs.rapids.ai/api/cuml/stable/) |
| NetworkX | cuGraph| [docs](https://docs.rapids.ai/api/cugraph/stable/) |
| dask | `dask_cuda`| [docs](https://docs.rapids.ai/api/dask-cuda/stable/) |
| spark | `spark-rapids` | [docs](https://nvidia.github.io/spark-rapids/) |

#### 1.2.2 


CUDA Deep Neural Network library (CuDNN)
CuDNN is a library of GPU accelerated primitives for deep neural networks. CuDNN provides implementations of standard routines such as forward or backward convolution, matmul, pooling and normalization.

Popular deep learning frameworks like Pytorch and Tensorflow already use CuDNN as part of their code.

CuDNN SDK's are available in `python` and `C++`, and is split into `frontend` and `backend` api's
- frontend - abstraction layer with the more common deep learning operations
- backend - lower level more legacy C level interface

#### 1.3 Nsight Developer tools



#### 1.4 CUDA Tile
Tile programming that is meant to be intuitive to python users




#### 1.5 CUDA Toolkit
Nvidia CUDA toolkit is a development environment that allows programmers to use GPUs for general purpose computing.

It provides essential tools such as `cuBLAS`, `cuDNN`, performance profilers, compiler (NVCC) and runtime

### 2. NVIDIA NGC Catalog
NGC is a portal of enterpise software managemet tools to support end-to-end AI and digital twin workflows

NGC consists of 
- NVIDIA NeMo - suite of tools to build, monitor and optimize agents across their lifecycles
- BioNemo - suite of tools for biotech research (clara family of open source models for biomed research)
- Riva Studio - customizable ai voice agents (automatic speech recognition, text to speech, neural machine translation) 
- NGC private registry
    - GPU optimized containers, models, helm charts


### 3. Triton Inference Server
Triton Inference server allows teams to deploy any AI model across frameworks and across devices. 

#### 3.1.1 Features
- Current frameworks supported: `TensorRT`, `Pytorch`, `ONNX`, `OpenVino`, `RAPIDS FIL`, `vLLM` and more
- Current devices supported: x86, ARM CPU AWS inferentia
- Built in `readiness` and `liveness` checks for integration into Kubernetes
    - can deploy Triton inference server with K8s with Helm on `GCP`, `AWS`, `Nvidia fleet command`
- Built in `HTTP/Rest` or `GRPC` endpoints

#### 3.2 Dynamo-Triton
The Triton inference server has been incorporated into Nvidia Dynamo platform with the new name Dynamo Triton.

##### 3.2.1 Dynamo Platform
NVIDIA Dynamo platform is a open source low latency and modular framework for serving generative AI models in distributed environments. 

Nvidia Dynamo supports AI inference backend (e.g. mistralrs, cuda) and features LLM specific optimzations (e.g. disaggregated serving)

Dynamo addresses the challenges of distributed and disaggregated inference serving via the 4 components
- GPU resource planning - Plans and schedules GPU resources in multi-node deployments to allocate them across pre-fill and decode
- Smart router - KV- cache aware routing engine that efficiently directs incoming traffic across large GPU fleets in multi-mode deployment
- Low latency communications library - inference data transfer library that accelerates the transfer of KV cache between GPUs and different memory and storage types
- KV cache manager - cost aware KV cache offloading engine designed to transfer KV cache across various memory hierachy freeing up GPU

![dynamo platform](./assets/dynamo_platform.png)

### 4. Robotics and Edge AI


#### 4.1 Embedded Devices
##### 4.1.1 NVIDIA Thor
Delivers over 2070 FP4 TFLOPS of AI compute and 128 GB of memory with power configurable between 40W and 130W

Comes with:
- NVIDIA ISSAC - robotics
    -  
- NVIDIA Metropolis - video analytics with AI agents
- NVIDIA Holoscan - sensor processing 

##### 4.1.2 NVIDIA Jetson Orin


#### 4.2 NVIDIA OMNIVERSE
A modular development platform of SDK and APIs for building 3D applications and services on Universal Scene Description (OpenUSD) and NVIDIA RTX (Real time raytracing)


